{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### RWSE-Checker: true-positives statistics from filtered corpus",
   "id": "421a2e9efb7631ae"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-20T13:36:05.460901Z",
     "start_time": "2025-01-20T13:36:04.249961Z"
    }
   },
   "source": "from rwse import RWSE_Checker",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgardner/GitHub/catalpa-cl/rwse-experiments/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:36:06.188348Z",
     "start_time": "2025-01-20T13:36:05.564760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rwse = RWSE_Checker()\n",
    "rwse.set_confusion_sets('../data/confusion_sets_modified.csv')"
   ],
   "id": "a0932c10ebc1e5af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/dgardner/GitHub/catalpa-cl/rwse-experiments/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Read from corpus and collect sentences by confusion sets",
   "id": "b15b220fc4c36fa6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:36:36.298480Z",
     "start_time": "2025-01-20T13:36:06.192012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../data/eng_news_2023_10K-sentences.txt', 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "sentences_cleaned = [sentence.split('\\t')[1].strip() for sentence in sentences]\n",
    "\n",
    "from util import collect_sentences_by_confusion_sets\n",
    "\n",
    "sentences_by_confusion_sets = collect_sentences_by_confusion_sets(rwse.confusion_sets.values(), sentences_cleaned)\n",
    "\n",
    "total = 0\n",
    "\n",
    "for key, value in sentences_by_confusion_sets.items():\n",
    "    total += len(value)\n",
    "    print(key, '=', len(value))\n",
    "\n",
    "print('total =', total)"
   ],
   "id": "ad7219e32c9680b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accept,except = 20\n",
      "advise,advice = 21\n",
      "affect,effect = 31\n",
      "begin,being = 213\n",
      "bitch,pitch = 7\n",
      "brakes,breaks = 7\n",
      "burrows,borrows = 0\n",
      "sight,site = 29\n",
      "cords,chords = 0\n",
      "country,county = 100\n",
      "crap,crab = 2\n",
      "dessert,desert = 6\n",
      "ease,easy = 30\n",
      "effects,affects = 23\n",
      "extend,extent = 16\n",
      "feet,feat = 21\n",
      "few,view = 103\n",
      "form,from = 839\n",
      "forth,fourth = 33\n",
      "forums,forms = 3\n",
      "fund,found = 115\n",
      "lead,led = 96\n",
      "life,live = 147\n",
      "loose,lose = 20\n",
      "mad,made = 154\n",
      "or,ore = 413\n",
      "passed,past = 80\n",
      "peace,piece = 28\n",
      "plane,plain = 12\n",
      "principal,principle = 13\n",
      "quite,quiet = 37\n",
      "raise,rise = 32\n",
      "safe,save = 48\n",
      "spit,split = 9\n",
      "than,then = 419\n",
      "their,there,they = 1421\n",
      "theme,them = 246\n",
      "things,thinks = 69\n",
      "trail,trial = 27\n",
      "tree,three = 174\n",
      "two,too,to = 4563\n",
      "weak,week = 121\n",
      "weather,whether = 56\n",
      "weed,wheat = 1\n",
      "where,were = 598\n",
      "which,witch = 424\n",
      "whole,hole = 33\n",
      "with,width = 1383\n",
      "world,word = 119\n",
      "you,your = 698\n",
      "total = 13060\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Switch confusion words in sentences",
   "id": "6449a8578e6ce09a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:38:51.255205Z",
     "start_time": "2025-01-20T13:36:36.308268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from util import replace_confusion_set_words_in_sentences\n",
    "modified_sentences_by_confusion_sets = replace_confusion_set_words_in_sentences(sentences_by_confusion_sets)\n",
    "total = 0\n",
    "\n",
    "for key, value in modified_sentences_by_confusion_sets.items():\n",
    "    total += len(value)\n",
    "    print(key, '=', len(value))\n",
    "\n",
    "print('total =', total)"
   ],
   "id": "15a303c49b3c3978",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accept,except = 20\n",
      "advise,advice = 21\n",
      "affect,effect = 32\n",
      "begin,being = 230\n",
      "bitch,pitch = 7\n",
      "brakes,breaks = 7\n",
      "burrows,borrows = 0\n",
      "sight,site = 29\n",
      "cords,chords = 0\n",
      "country,county = 104\n",
      "crap,crab = 2\n",
      "dessert,desert = 6\n",
      "ease,easy = 30\n",
      "effects,affects = 23\n",
      "extend,extent = 16\n",
      "feet,feat = 22\n",
      "few,view = 104\n",
      "form,from = 898\n",
      "forth,fourth = 34\n",
      "forums,forms = 3\n",
      "fund,found = 118\n",
      "lead,led = 100\n",
      "life,live = 153\n",
      "loose,lose = 20\n",
      "mad,made = 156\n",
      "or,ore = 443\n",
      "passed,past = 80\n",
      "peace,piece = 28\n",
      "plane,plain = 13\n",
      "principal,principle = 13\n",
      "quite,quiet = 39\n",
      "raise,rise = 32\n",
      "safe,save = 50\n",
      "spit,split = 9\n",
      "than,then = 435\n",
      "their,there,they = 3730\n",
      "theme,them = 259\n",
      "things,thinks = 69\n",
      "trail,trial = 27\n",
      "tree,three = 176\n",
      "two,too,to = 12638\n",
      "weak,week = 128\n",
      "weather,whether = 56\n",
      "weed,wheat = 1\n",
      "where,were = 671\n",
      "which,witch = 437\n",
      "whole,hole = 33\n",
      "with,width = 1491\n",
      "world,word = 122\n",
      "you,your = 1149\n",
      "total = 24264\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Determine RWSEs",
   "id": "f1becca5e6105f9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T13:51:59.058197Z",
     "start_time": "2025-01-20T13:38:51.295789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from cassis import Cas, load_typesystem\n",
    "import spacy\n",
    "\n",
    "T_SENTENCE = 'de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence'\n",
    "T_RWSE = 'de.tudarmstadt.ukp.dkpro.core.api.anomaly.type.RWSE'\n",
    "T_TOKEN = 'de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token'\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "path = '../data/TypeSystem.xml'\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    ts = load_typesystem(f)\n",
    "\n",
    "S = ts.get_type(T_SENTENCE)\n",
    "T = ts.get_type(T_TOKEN)\n",
    "\n",
    "result = dict()\n",
    "\n",
    "for confusion_set, sentences in modified_sentences_by_confusion_sets.items():\n",
    "    cas = Cas(ts)\n",
    "    if len(sentences) > 6000:\n",
    "        print('WARNING: too many sentences:', len(sentences), 'for', confusion_set)\n",
    "        continue\n",
    "    cas.sofa_string = ' '.join(sentences)\n",
    "    doc = nlp(cas.sofa_string)\n",
    "    for sent in doc.sents:\n",
    "        cas_sentence = S(begin=sent.start_char, end=sent.end_char)\n",
    "        cas.add(cas_sentence)\n",
    "    for token in doc:\n",
    "        cas_token = T(begin=token.idx, end=token.idx+len(token.text), id=token.i)\n",
    "        cas.add(cas_token)\n",
    "    rwse.set_confusion_sets([set(confusion_set.split(','))])\n",
    "    rwse.check_cas(cas, ts)\n",
    "    true_positives = cas.select(T_RWSE)\n",
    "    with open('../experiments/data/true_positives.csv', 'a') as f:\n",
    "        false_positives_transformed = [(token.begin, token.end, cas.sofa_string[token.begin:token.end], token.suggestion) for token in true_positives]\n",
    "        for sent in doc.sents:\n",
    "            for token in false_positives_transformed:\n",
    "                if sent.start_char <= token[0] and sent.end_char >= token[1]:\n",
    "                    print(confusion_set, f'{token[2]}->{token[3]}',sent.text, sep='\\t', file=f)\n",
    "    print(confusion_set, \":\", len(true_positives), 'from', len(sentences))\n",
    "    diff = len(sentences)-len(true_positives)\n",
    "    if diff != 0:\n",
    "        print(f\"WARNING: {diff} missed matches for {confusion_set}\")\n",
    "    result[confusion_set] = {\n",
    "        'num_sentences': len(sentences),\n",
    "        'num_matches': len(true_positives)\n",
    "    }"
   ],
   "id": "2db82ca79085232b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accept,except : 19 from 20\n",
      "WARNING: 1 missed matches for accept,except\n",
      "advise,advice : 21 from 21\n",
      "affect,effect : 30 from 32\n",
      "WARNING: 2 missed matches for affect,effect\n",
      "begin,being : 230 from 230\n",
      "bitch,pitch : 7 from 7\n",
      "brakes,breaks : 7 from 7\n",
      "burrows,borrows : 0 from 0\n",
      "sight,site : 28 from 29\n",
      "WARNING: 1 missed matches for sight,site\n",
      "cords,chords : 0 from 0\n",
      "country,county : 93 from 104\n",
      "WARNING: 11 missed matches for country,county\n",
      "crap,crab : 1 from 2\n",
      "WARNING: 1 missed matches for crap,crab\n",
      "dessert,desert : 6 from 6\n",
      "ease,easy : 29 from 30\n",
      "WARNING: 1 missed matches for ease,easy\n",
      "effects,affects : 23 from 23\n",
      "extend,extent : 16 from 16\n",
      "feet,feat : 21 from 22\n",
      "WARNING: 1 missed matches for feet,feat\n",
      "few,view : 104 from 104\n",
      "form,from : 893 from 898\n",
      "WARNING: 5 missed matches for form,from\n",
      "forth,fourth : 34 from 34\n",
      "forums,forms : 3 from 3\n",
      "fund,found : 116 from 118\n",
      "WARNING: 2 missed matches for fund,found\n",
      "lead,led : 96 from 100\n",
      "WARNING: 4 missed matches for lead,led\n",
      "life,live : 147 from 153\n",
      "WARNING: 6 missed matches for life,live\n",
      "loose,lose : 19 from 20\n",
      "WARNING: 1 missed matches for loose,lose\n",
      "mad,made : 156 from 156\n",
      "or,ore : 444 from 443\n",
      "WARNING: -1 missed matches for or,ore\n",
      "passed,past : 79 from 80\n",
      "WARNING: 1 missed matches for passed,past\n",
      "peace,piece : 27 from 28\n",
      "WARNING: 1 missed matches for peace,piece\n",
      "plane,plain : 12 from 13\n",
      "WARNING: 1 missed matches for plane,plain\n",
      "principal,principle : 12 from 13\n",
      "WARNING: 1 missed matches for principal,principle\n",
      "quite,quiet : 39 from 39\n",
      "raise,rise : 27 from 32\n",
      "WARNING: 5 missed matches for raise,rise\n",
      "safe,save : 52 from 50\n",
      "WARNING: -2 missed matches for safe,save\n",
      "spit,split : 9 from 9\n",
      "than,then : 426 from 435\n",
      "WARNING: 9 missed matches for than,then\n",
      "their,there,they : 3621 from 3730\n",
      "WARNING: 109 missed matches for their,there,they\n",
      "theme,them : 257 from 259\n",
      "WARNING: 2 missed matches for theme,them\n",
      "things,thinks : 68 from 69\n",
      "WARNING: 1 missed matches for things,thinks\n",
      "trail,trial : 24 from 27\n",
      "WARNING: 3 missed matches for trail,trial\n",
      "tree,three : 175 from 176\n",
      "WARNING: 1 missed matches for tree,three\n",
      "WARNING: too many sentences: 12638 for two,too,to\n",
      "weak,week : 126 from 128\n",
      "WARNING: 2 missed matches for weak,week\n",
      "weather,whether : 56 from 56\n",
      "weed,wheat : 1 from 1\n",
      "where,were : 660 from 671\n",
      "WARNING: 11 missed matches for where,were\n",
      "which,witch : 438 from 437\n",
      "WARNING: -1 missed matches for which,witch\n",
      "whole,hole : 32 from 33\n",
      "WARNING: 1 missed matches for whole,hole\n",
      "with,width : 1492 from 1491\n",
      "WARNING: -1 missed matches for with,width\n",
      "world,word : 118 from 122\n",
      "WARNING: 4 missed matches for world,word\n",
      "you,your : 1039 from 1149\n",
      "WARNING: 110 missed matches for you,your\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Seperate computation of confusion set two,too,to",
   "id": "29a6fd7cf07758cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T14:28:56.355677Z",
     "start_time": "2025-01-20T14:11:26.031940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "confusion_set = 'two,too,to'\n",
    "org_sentences = modified_sentences_by_confusion_sets[confusion_set]\n",
    "for sentences in [org_sentences[:6500], org_sentences[6500:]]:\n",
    "    cas = Cas(ts)\n",
    "    cas.sofa_string = ' '.join(sentences)\n",
    "    doc = nlp(cas.sofa_string)\n",
    "    for sent in doc.sents:\n",
    "        cas_sentence = S(begin=sent.start_char, end=sent.end_char)\n",
    "        cas.add(cas_sentence)\n",
    "    for token in doc:\n",
    "        cas_token = T(begin=token.idx, end=token.idx+len(token.text), id=token.i)\n",
    "        cas.add(cas_token)\n",
    "    rwse.set_confusion_sets([set(confusion_set.split(','))])\n",
    "    rwse.check_cas(cas, ts)\n",
    "    true_positives = cas.select(T_RWSE)\n",
    "    with open('../experiments/data/true_positives.csv', 'a') as f:\n",
    "        false_positives_transformed = [(token.begin, token.end, cas.sofa_string[token.begin:token.end], token.suggestion) for token in true_positives]\n",
    "        for sent in doc.sents:\n",
    "            for token in false_positives_transformed:\n",
    "                if sent.start_char <= token[0] and sent.end_char >= token[1]:\n",
    "                    print(confusion_set, f'{token[2]}->{token[3]}',sent.text, sep='\\t', file=f)\n",
    "    print(confusion_set, \":\", len(true_positives), 'from', len(sentences))\n",
    "    diff = len(sentences)-len(true_positives)\n",
    "    if diff != 0:\n",
    "        print(f\"WARNING: {diff} missed matches for {confusion_set}\")\n",
    "    result[confusion_set] = {\n",
    "        'num_sentences': len(sentences),\n",
    "        'num_matches': len(true_positives)\n",
    "    }"
   ],
   "id": "ecefcd12bc22eb0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two,too,to : 6490 from 6500\n",
      "WARNING: 10 missed matches for two,too,to\n",
      "two,too,to : 6130 from 6138\n",
      "WARNING: 8 missed matches for two,too,to\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T14:02:06.545405Z",
     "start_time": "2025-01-20T14:02:06.542009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total = sum(item['num_sentences'] for item in result.values())\n",
    "total_matches = sum(item['num_matches'] for item in result.values())\n",
    "print(f'true positive rate: {total_matches/total:.2f}')"
   ],
   "id": "5ee3249bf547ef32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positive rate: 0.97\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
