{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421a2e9efb7631ae",
   "metadata": {},
   "source": [
    "### RWSE-Checker: false-positives (false alarm) statistics from filtered corpus"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T21:27:39.098448Z",
     "start_time": "2025-02-02T21:27:37.450934Z"
    }
   },
   "source": "from rwse import RWSE_Checker",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgardner/GitHub/catalpa-cl/rwse-experiments/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a0932c10ebc1e5af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T21:27:39.977723Z",
     "start_time": "2025-02-02T21:27:39.103304Z"
    }
   },
   "source": [
    "rwse = RWSE_Checker()\n",
    "rwse.set_confusion_sets('input/confusion_sets_modified.csv')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "b15b220fc4c36fa6",
   "metadata": {},
   "source": "#### Load sentences by confusion sets"
  },
  {
   "cell_type": "code",
   "id": "ad7219e32c9680b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T21:27:40.136726Z",
     "start_time": "2025-02-02T21:27:40.129679Z"
    }
   },
   "source": [
    "file_name = 'input/eng_news_2023-balanced-sentences.csv'\n",
    "\n",
    "sentences_by_confusion_sets = dict()\n",
    "with open(file_name, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        confusion_set, sentence = line.strip().split('\\t')\n",
    "        if sentences_by_confusion_sets.get(confusion_set) is None:\n",
    "            sentences_by_confusion_sets[confusion_set] = []\n",
    "        sentences_by_confusion_sets[confusion_set].append(sentence)\n",
    "\n",
    "total = 0\n",
    "\n",
    "for key, value in sorted(sentences_by_confusion_sets.items()):\n",
    "    total += len(value)\n",
    "    print(key, '=', len(value))\n",
    "\n",
    "print('total =', total)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Being,begin,being = 100\n",
      "Capital,Capitol,capital = 100\n",
      "Country,County,country,county = 100\n",
      "Desert,desert,dessert = 100\n",
      "Easy,ease,easy = 100\n",
      "Effect,affect,effect = 100\n",
      "Effects,affects,effects = 100\n",
      "Except,accept,except = 100\n",
      "Few,View,few,view = 100\n",
      "Form,From,form,from = 100\n",
      "Found,Fund,found,fund = 100\n",
      "Fourth,forth,fourth = 100\n",
      "Hole,Whole,hole,whole = 100\n",
      "Lead,Led,lead,led = 100\n",
      "Life,Live,life,live = 100\n",
      "Mad,Made,mad,made = 100\n",
      "Or,or,ore = 94\n",
      "Past,passed,past = 100\n",
      "Peace,Piece,peace,piece = 100\n",
      "Plain,plain,plane = 100\n",
      "Principal,principal,principle = 100\n",
      "Provence,Province,province = 12\n",
      "Quiet,Quite,quiet,quite = 100\n",
      "Rise,raise,rise = 100\n",
      "Safe,Save,safe,save = 100\n",
      "Site,sight,site = 100\n",
      "Split,spit,split = 59\n",
      "Than,Then,than,then = 100\n",
      "Their,There,They,their,there,they = 100\n",
      "Them,Theme,them,theme = 100\n",
      "Things,things,thinks = 100\n",
      "Three,Tree,three,tree = 100\n",
      "To,Too,Two,to,too,two = 100\n",
      "Trail,Trial,trail,trial = 100\n",
      "Weather,Whether,weather,whether = 100\n",
      "Week,weak,week = 100\n",
      "Were,Where,were,where = 100\n",
      "Which,Witch,which,witch = 100\n",
      "With,width,with = 65\n",
      "Word,World,word,world = 100\n",
      "You,Your,you,your = 100\n",
      "advice,advise = 100\n",
      "bitch,pitch = 36\n",
      "brakes,breaks = 60\n",
      "crab,crap = 59\n",
      "extend,extent = 100\n",
      "feat,feet = 100\n",
      "forms,forums = 86\n",
      "loose,lose = 100\n",
      "weed,wheat = 100\n",
      "total = 4671\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "f1becca5e6105f9f",
   "metadata": {},
   "source": [
    "#### Determine RWSEs"
   ]
  },
  {
   "cell_type": "code",
   "id": "757c9a6da4c9b8b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T21:27:40.773099Z",
     "start_time": "2025-02-02T21:27:40.143150Z"
    }
   },
   "source": [
    "from cassis import Cas, load_typesystem\n",
    "import spacy\n",
    "\n",
    "T_SENTENCE = 'de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Sentence'\n",
    "T_RWSE = 'de.tudarmstadt.ukp.dkpro.core.api.anomaly.type.RWSE'\n",
    "T_TOKEN = 'de.tudarmstadt.ukp.dkpro.core.api.segmentation.type.Token'\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "path = 'input/TypeSystem.xml'\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    ts = load_typesystem(f)\n",
    "\n",
    "S = ts.get_type(T_SENTENCE)\n",
    "T = ts.get_type(T_TOKEN)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "2db82ca79085232b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T21:31:32.261054Z",
     "start_time": "2025-02-02T21:27:40.777897Z"
    }
   },
   "source": [
    "file_name = 'output/false_positives_balanced.csv'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "\n",
    "    result = dict()\n",
    "\n",
    "    for confusion_set, sentences in sentences_by_confusion_sets.items():\n",
    "        matches = 0\n",
    "        rwse.set_confusion_sets([set(confusion_set.split(','))])\n",
    "        for sentence in sentences:\n",
    "            cas = Cas(ts)\n",
    "            # TODO clean sentence?\n",
    "            cas.sofa_string = sentence\n",
    "            doc = nlp(cas.sofa_string)\n",
    "            cas_sentence = S(begin=0, end=len(sentence))\n",
    "            cas.add(cas_sentence)\n",
    "            for token in doc:\n",
    "                cas_token = T(begin=token.idx, end=token.idx+len(token.text), id=token.i)\n",
    "                cas.add(cas_token)\n",
    "            rwse.check_cas(cas, ts)\n",
    "            false_positives = cas.select(T_RWSE)\n",
    "            if len(false_positives) != 0:\n",
    "                matches += 1\n",
    "                for item in false_positives:\n",
    "                    before = cas.sofa_string[item.begin:item.end]\n",
    "                    modified_string = cas.sofa_string[:item.begin] + ' [[' + cas.sofa_string[item.begin:item.end] + ']] ' +cas.sofa_string[item.end:]\n",
    "                    print(f'{before} => {item.suggestion}' ,f'({item.certainty:.5f})' , modified_string, sep='\\t', file=f)\n",
    "        result[confusion_set] = {\n",
    "            'num_sentences':len(sentences),\n",
    "            'num_matches':matches,\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "93f59c11ebde61c2",
   "metadata": {},
   "source": [
    "#### Determine false-positive rate"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0b3814b18fa75fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T21:31:32.278016Z",
     "start_time": "2025-02-02T21:31:32.275100Z"
    }
   },
   "source": [
    "total = sum(item['num_sentences'] for item in result.values())\n",
    "total_matches = sum(item['num_matches'] for item in result.values())\n",
    "print(f'false positive rate: {total_matches/total:.2f}')\n",
    "print(f'falsely identified {total_matches} out of {total}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positive rate: 0.00\n",
      "falsely identified 16 out of 4671\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "327a5af8756a2143",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T21:31:32.299127Z",
     "start_time": "2025-02-02T21:31:32.295672Z"
    }
   },
   "source": [
    "file_name = 'output/report_false_positives_balanced.csv'\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "    print('confusion_set', 'num_matches', 'num_sentences', sep=';', file=f)\n",
    "    for key, value in result.items():\n",
    "        print(key, value['num_matches'], value['num_sentences'], sep=';', file=f)\n",
    "        print(key, value['num_matches'], value['num_sentences'], sep='\\t')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Except,accept,except\t0\t100\n",
      "advice,advise\t0\t100\n",
      "Effect,affect,effect\t2\t100\n",
      "Being,begin,being\t0\t100\n",
      "bitch,pitch\t0\t36\n",
      "brakes,breaks\t1\t60\n",
      "Capital,Capitol,capital\t0\t100\n",
      "Site,sight,site\t0\t100\n",
      "Country,County,country,county\t1\t100\n",
      "crab,crap\t0\t59\n",
      "Desert,desert,dessert\t1\t100\n",
      "Easy,ease,easy\t1\t100\n",
      "Effects,affects,effects\t1\t100\n",
      "extend,extent\t0\t100\n",
      "feat,feet\t0\t100\n",
      "Few,View,few,view\t0\t100\n",
      "Form,From,form,from\t0\t100\n",
      "Fourth,forth,fourth\t0\t100\n",
      "forms,forums\t1\t86\n",
      "Found,Fund,found,fund\t0\t100\n",
      "Lead,Led,lead,led\t0\t100\n",
      "Life,Live,life,live\t0\t100\n",
      "loose,lose\t1\t100\n",
      "Mad,Made,mad,made\t0\t100\n",
      "Or,or,ore\t0\t94\n",
      "Past,passed,past\t0\t100\n",
      "Peace,Piece,peace,piece\t0\t100\n",
      "Plain,plain,plane\t1\t100\n",
      "Principal,principal,principle\t1\t100\n",
      "Provence,Province,province\t0\t12\n",
      "Quiet,Quite,quiet,quite\t0\t100\n",
      "Rise,raise,rise\t1\t100\n",
      "Safe,Save,safe,save\t1\t100\n",
      "Split,spit,split\t0\t59\n",
      "Than,Then,than,then\t0\t100\n",
      "Their,There,They,their,there,they\t1\t100\n",
      "Them,Theme,them,theme\t0\t100\n",
      "Things,things,thinks\t0\t100\n",
      "Trail,Trial,trail,trial\t1\t100\n",
      "Three,Tree,three,tree\t0\t100\n",
      "To,Too,Two,to,too,two\t0\t100\n",
      "Week,weak,week\t0\t100\n",
      "Weather,Whether,weather,whether\t0\t100\n",
      "weed,wheat\t1\t100\n",
      "Were,Where,were,where\t0\t100\n",
      "Which,Witch,which,witch\t0\t100\n",
      "Hole,Whole,hole,whole\t0\t100\n",
      "With,width,with\t0\t65\n",
      "Word,World,word,world\t0\t100\n",
      "You,Your,you,your\t0\t100\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T21:31:32.318066Z",
     "start_time": "2025-02-02T21:31:32.316690Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a135b62badf9f4a9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
